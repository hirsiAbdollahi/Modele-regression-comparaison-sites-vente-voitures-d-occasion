{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tools :\n",
    "import numpy as np                      \n",
    "import pandas as pd  \n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt         \n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder,StandardScaler,LabelBinarizer \n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Predictors :\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, LogisticRegressionCV, RidgeCV, LassoCV, ElasticNet\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "# from sklearn import neighbors \n",
    "# from sklearn.naive_bayes import GaussianNB , ComplementNB,CategoricalNB              \n",
    "\n",
    "# Metrics : \n",
    "from sklearn.metrics import mean_squared_error, r2_score,roc_curve, roc_auc_score, auc \n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "# Optimization / Validation :\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn import svm, datasets,preprocessing\n",
    "\n",
    "# cell multiple outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Others :\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATIONS\n",
    "\n",
    "def plot_matrix(df, seuil, target):\n",
    "\n",
    "    corr_mask = abs(df.corr()[target]) > seuil\n",
    "    high_corr = df.corr().loc[corr_mask,corr_mask]\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.heatmap(high_corr, \n",
    "                annot=True, \n",
    "                cmap=\"bwr\",\n",
    "                linecolor='white', \n",
    "                linewidths='1', \n",
    "                square=True,\n",
    "                center=0, \n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "                mask = np.triu(np.ones_like(df.corr(), dtype=np.bool)),\n",
    "                fmt='.2g')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    print(high_corr.shape)\n",
    "    \n",
    "# plot_matrix(beke, 0,'prix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File lacentrale_clean4.csv does not exist: 'lacentrale_clean4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-91c835fe440d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlacentrale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lacentrale_clean4.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbeke\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'beke_processed.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File lacentrale_clean4.csv does not exist: 'lacentrale_clean4.csv'"
     ]
    }
   ],
   "source": [
    "lacentrale = pd.read_csv('lacentrale_clean4.csv')\n",
    "beke = pd.read_csv('beke_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lacentrale.mise_circulation=lacentrale.mise_circulation.astype(str)\n",
    "lacentrale.conso_mixte=lacentrale.conso_mixte.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "\n",
    "    # on définit les colonnes et les transformations pour \n",
    "    # les colonnes quantitatives\n",
    "    col_quanti=['nb_km','nb_portes','nb_places','conso_mixte','p_fiscale','p_din']\n",
    "\n",
    "    transfo_quanti = Pipeline(steps=[\n",
    "        ('imputation', SimpleImputer(strategy='median')),\n",
    "        ('standard', StandardScaler())])\n",
    "\n",
    "    # on définit les colonnes et les transformations pour\n",
    "    # les variables qualitatives\n",
    "\n",
    "    #variable ordinales\n",
    "    #le mapping = ordinal_cols_mapping\n",
    "    ordinal_cols_mapping = [\n",
    "        {\n",
    "        \"col\":'co2',    \n",
    "        \"mapping\": {\n",
    "            'A':4, \n",
    "            'B':3, \n",
    "            'C':2, \n",
    "            'D':1,       \n",
    "        }},\n",
    "\n",
    "        {\n",
    "        \"col\":'critair',    \n",
    "        \"mapping\": {\n",
    "            1:3, \n",
    "            2:2, \n",
    "            3:1,       \n",
    "        }} \n",
    "    ]\n",
    "\n",
    "\n",
    "    col_quali_ord = ['co2','critair',]\n",
    "\n",
    "\n",
    "    transfo_quali_ord = Pipeline(steps=[\n",
    "        ('ordinalEncoder', OrdinalEncoder(mapping=ordinal_cols_mapping, return_df = True))\n",
    "    ])\n",
    "\n",
    "    col_quali_nom= ['marque','modele','categorie','carburant','boite_vitesse','couleur','mise_circulation' ]\n",
    "\n",
    "\n",
    "    transfo_quali_nom = Pipeline(steps=[\n",
    "        ('imputation', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # on définit l'objet de la classe ColumnTransformer\n",
    "    # qui va permettre d'appliquer toutes les étapes\n",
    "\n",
    "    preparation = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('quanti', transfo_quanti , col_quanti),\n",
    "            ('quali_ord', transfo_quali_ord , col_quali_ord),\n",
    "            ('quali_nom', transfo_quali_nom , col_quali_nom)\n",
    "        ], remainder='drop')\n",
    "    \n",
    "    return preparation \n",
    "\n",
    "preparation = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = {\n",
    "        \n",
    "       'RF': { 'model':RandomForestRegressor(),\n",
    "              'param':{\n",
    "                  'clf__n_estimators': [100,200,300,500,1000],\n",
    "                'clf__max_depth': [1,5,10,15,50,70],\n",
    "#                   'clf__min_samples_split': [1,5,10,15]\n",
    "#                   'cl_max_leaf_nodes': [ 100, 200, 300, 400, 500, 600, 650, 700, 800]\n",
    "                  },\n",
    "             },\n",
    "        \n",
    "\n",
    "        'Lasso': { 'model': LassoCV(),\n",
    "                'param': {'clf__alpha':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "             },\n",
    "    \n",
    "        'Ridge': { 'model': RidgeCV(),\n",
    "                'param': {'clf__alpha':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "             },\n",
    "    \n",
    "        'Elastic': { 'model': ElasticNet(),\n",
    "                'param': {'clf__alpha':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                         'clf__l1_ratio' : [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "                         }\n",
    "             },\n",
    "    \n",
    "        'LR': { 'model': LogisticRegression(),\n",
    "                'param': {'clf__penalty' : ['l1', 'l2'], 'clf__C' : np.logspace(-4, 4, 20), 'clf__solver' : ['liblinear']},\n",
    "             },\n",
    "       \n",
    "        'SVR':{ 'model': SVR(),\n",
    "                'param': {'clf__C': [0.1,1, 10, 100], 'clf__gamma': [1,0.1,0.01,0.001],'clf__kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "                         },\n",
    "             },\n",
    "      \n",
    "        'XGB':{ \"model\":XGBRegressor(),\n",
    "              \"param\":{\"clf__learning_rate\": [0.05,1,5],'clf__n_estimators': [100,50],\n",
    "#                        \"clf__max_depth\": [5,10,15]\n",
    "                  },\n",
    "            },\n",
    "    \n",
    "        'GradientBoost':{ \"model\":GradientBoostingRegressor(),\n",
    "              \"param\":{\"clf__model__n_estimators\": [ 500, 600,700,800,1000],\n",
    "#                         \"clf__max_depth\": [2, 3, 4]\n",
    "                  },\n",
    "            },\n",
    "    \n",
    "        'decisionTree':{ \"model\":GradientBoostingRegressor(),\n",
    "              \"param\":{\"clf__criterion\": ['mse', 'mae'],\n",
    "                'clf__min_samples_leaf': [5, 10, 15, 20, 25],\n",
    "                'clf__max_depth': [6, 9, 12, 15, 20],\n",
    "                  },\n",
    "            },\n",
    "         \n",
    "        \n",
    "}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modele_entier(cl, df_train, df_test):\n",
    "    model = Pipeline(steps=[('preparation', preparation),\n",
    "                            ('to_dense', DenseTransformer()),\n",
    "                         ('clf', cl['model'] )\n",
    "                    ])\n",
    "    \n",
    "    # PROCESSING & ENTRAINEMENT\n",
    "\n",
    "    param_grid = cl['param']\n",
    "\n",
    "    # on sépare la cible du reste des données (dataset d'entraînement)\n",
    "    X = df_train.drop(['prix','nom','ref'], axis=1)\n",
    "    y = df_train['prix']\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42)\n",
    "    # Debut du decompte du temps\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10, verbose=False)\n",
    "    \n",
    "    grid.fit(X_train,y_train)\n",
    "    \n",
    "    print(grid.best_estimator_)\n",
    "    \n",
    "    # Score de l'entraînement\n",
    "    accuracy = grid.score(X_test, y_test)    \n",
    "    print((\"best score  : %.5f\" % accuracy))\n",
    "    \n",
    "    # Temps d'entraînement\n",
    "    times = (time.time() - start_time)\n",
    "    print(\"Temps d'entraînement' : %s secondes ---\" % times)    \n",
    "    \n",
    "    # PREDICTIONS\n",
    "    \n",
    "    # on sépare la cible du reste des données (dataset de test)\n",
    "    X_reel = df_test.drop(['prix','nom','ref'], axis=1)\n",
    "    y_reel = df_test['prix']  \n",
    "    \n",
    "    y_pred = grid.predict(X_reel)   \n",
    "                                    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = modele_entier(regression['SVR'], lacentrale, beke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    \"prix_prediction\": y_pred,\n",
    "    \"prix_beke\":beke['prix'].values,\n",
    "    \"p_beke-p_pred\":beke['prix'].values - y_pred,\n",
    "}\n",
    "\n",
    "comparaison = pd.DataFrame(data=dic)\n",
    "comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
